{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fight-Online-Abuse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from transformers import AdamW, DistilBertModel, DistilBertTokenizerFast\n",
    "\n",
    "from datasets import TextDataset\n",
    "from models import DistilBertClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = Path('./datasets')\n",
    "RESULT_DIR = Path('./results')\n",
    "CHECKPOINT_DIR = Path('./checkpoints')\n",
    "RESULT_DIR.mkdir(exist_ok=True)\n",
    "CHECKPOINT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "TRAIN_SPLIT = 0.8\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-05\n",
    "EPOCHS = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories: ['toxic' 'severe_toxic' 'obscene' 'threat' 'insult' 'identity_hate']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0000997932d777bf</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000103f0d9cfb60f</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000113f07ec002fd</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0001b41b1c6bb37e</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0001d958c54c6e35</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       comment_text  toxic  \\\n",
       "id                                                                           \n",
       "0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "                  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "id                                                                      \n",
       "0000997932d777bf             0        0       0       0              0  \n",
       "000103f0d9cfb60f             0        0       0       0              0  \n",
       "000113f07ec002fd             0        0       0       0              0  \n",
       "0001b41b1c6bb37e             0        0       0       0              0  \n",
       "0001d958c54c6e35             0        0       0       0              0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(DATASET_DIR / 'train.csv', index_col=0)\n",
    "categories = df.columns[1:].values\n",
    "print(f\"Categories: {categories}\")\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "seq_len = [len(x.split()) for x in df[\"comment_text\"].values]\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.hist(seq_len, bins=30)\n",
    "plt.grid(True)\n",
    "plt.title(\"Sequence Length Distribution\")\n",
    "plt.xlabel(\"Sequence Length\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.savefig(RESULT_DIR / \"sequence_length_distribution.png\", facecolor='w')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 127656, Valid size: 31915\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "dataset = TextDataset(df['comment_text'], df[categories].values, tokenizer, MAX_LEN)\n",
    "\n",
    "train_size = int(TRAIN_SPLIT * len(dataset))\n",
    "valid_size = len(dataset) - train_size\n",
    "train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)}, Valid size: {len(valid_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_class_weights(y):\n",
    "    class_count = y.sum(axis=0)\n",
    "    class_weights = [len(y) / (len(y[0]) * freq) if freq > 0 else 1 for freq in class_count]\n",
    "\n",
    "    return class_weights\n",
    "\n",
    "class_weights = generate_class_weights(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertClassifier(\n",
      "  (bert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (1): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (2): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (3): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (4): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (5): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (relu): ReLU()\n",
      "  (fc1): Linear(in_features=768, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=6, bias=True)\n",
      "  (softmax): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python39\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "distilBert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# ? Freeze all the parameters\n",
    "for param in distilBert.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model = DistilBertClassifier(distilBert, len(categories)).to(device)\n",
    "print(model)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.NLLLoss()\n",
    "# criterion = nn.BCEWithLogitsLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ? tokenize and encode sequences\n",
    "# tokens_train = tokenizer(X_train.tolist(), max_length=256, padding='max_length', truncation=True)\n",
    "# tokens_valid = tokenizer(X_valid.tolist(), max_length=256, padding='max_length', truncation=True)\n",
    "\n",
    "# train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "# train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "# train_y = torch.tensor(y_train.tolist())\n",
    "\n",
    "# valid_seq = torch.tensor(tokens_valid['input_ids'])\n",
    "# valid_mask = torch.tensor(tokens_valid['attention_mask'])\n",
    "# valid_y = torch.tensor(y_valid.tolist())\n",
    "\n",
    "# # ? tokenize and encode sequences\n",
    "# tokens_train = tokenizer(X_train.tolist(), max_length=256, padding='max_length', truncation=True)\n",
    "# tokens_valid = tokenizer(X_valid.tolist(), max_length=256, padding='max_length', truncation=True)\n",
    "\n",
    "# train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "# train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "# train_y = torch.tensor(y_train.tolist())\n",
    "\n",
    "# valid_seq = torch.tensor(tokens_valid['input_ids'])\n",
    "# valid_mask = torch.tensor(tokens_valid['attention_mask'])\n",
    "# valid_y = torch.tensor(y_valid.tolist())\n",
    "\n",
    "\n",
    "# train_dataset = TensorDataset(train_seq, train_mask, train_y)\n",
    "# train_sampler = RandomSampler(train_dataset)\n",
    "# train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "# valid_dataset = TensorDataset(valid_seq, valid_mask, valid_y)\n",
    "# valid_sampler = SequentialSampler(valid_dataset)\n",
    "# valid_dataloader = DataLoader(valid_dataset, sampler=valid_sampler, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'token_type_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-48d5b8ba9eff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mavg_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_preds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_preds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-58-48d5b8ba9eff>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, dataloader, criterion, optimizer, device)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-49-ce7aea0622f1>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, ids, mask, token_type_ids)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;31m# ? pass the inputs to the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls_hs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls_hs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'token_type_ids'"
     ]
    }
   ],
   "source": [
    "def train(model, dataloader, criterion, optimizer, device='cpu'):\n",
    "    print(\"Training...\")\n",
    "    model.train()\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "\n",
    "    # ? empty list to save model predictions\n",
    "    total_preds = []\n",
    "\n",
    "    # ? iterate over batches\n",
    "    for idx, batch in enumerate(dataloader):\n",
    "        ids = batch['ids'].to(device)\n",
    "        mask = batch['mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n",
    "        print(preds)\n",
    "\n",
    "        # ? compute the loss between actual and predicted values\n",
    "    #     loss = criterion(preds, labels)\n",
    "\n",
    "    #     # add on to the total loss\n",
    "    #     total_loss = total_loss + loss.item()\n",
    "\n",
    "    #     # backward pass to calculate the gradients\n",
    "    #     loss.backward()\n",
    "\n",
    "        # ? clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # # model predictions are stored on GPU. So, push it to CPU\n",
    "        # preds = preds.detach().cpu().numpy()\n",
    "\n",
    "        # # append the model predictions\n",
    "        # total_preds.append(preds)\n",
    "\n",
    "        # ? progress update after every 50 batches.\n",
    "        if (idx + 1) % 50 == 0:\n",
    "            # print(f\"  [Batch {idx+1}\\t/{len(dataloader)}] Loss: {loss.item():.4f}\")\n",
    "            print(f\"  [Batch {idx+1}\\t/{len(dataloader)}]\")\n",
    "\n",
    "    # ? compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "\n",
    "    # # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "    # # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    # total_preds = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    # # returns the loss and predictions\n",
    "    return avg_loss, total_preds\n",
    "\n",
    "train_loss, train_preds = train(model, train_dataloader, criterion, optimizer, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for evaluating the model\n",
    "def evaluate(dataloader, criterion, device):\n",
    "    print(\"\\nEvaluating...\")\n",
    "    model.eval()\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "\n",
    "    # empty list to save the model predictions\n",
    "    total_preds = []\n",
    "\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        # Progress update every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "\n",
    "            # Report progress.\n",
    "            print(\"  Batch {:>5,}  of  {:>5,}.\".format(step, len(dataloader)))\n",
    "\n",
    "        # push the batch to gpu\n",
    "        batch = [t.to(device) for t in batch]\n",
    "\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        # deactivate autograd\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # model predictions\n",
    "            preds = model(sent_id, mask)\n",
    "\n",
    "            # compute the validation loss between actual and predicted values\n",
    "            loss = criterion(preds, labels)\n",
    "\n",
    "            total_loss = total_loss + loss.item()\n",
    "\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "\n",
    "            total_preds.append(preds)\n",
    "\n",
    "    # compute the validation loss of the epoch\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    return avg_loss, total_preds\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
